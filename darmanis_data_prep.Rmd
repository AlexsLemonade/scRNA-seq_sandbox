---
title: "Darmanis_data_prep"
author: "C. Savonen, CCDL for ALSF"
date: "12/6/2018"
output: html_document
---

## Purpose: Prep data for use in post-processing pipeline or in 
[ASAP online](https://asap.epfl.ch/)
```{r Import functions}
source(file.path("scripts", "util", "data_prep_functions.R"))

# Magrittr pipe
`%>%` <- dplyr::`%>%`
```

```{r Import data}
# Import gene expression matrix data
dataset <- readr::read_tsv(file.path("darmanis_data", "darmanis_counts.tsv"), 
                           progress = FALSE, guess_max = 10000)
```

### Import metadata
```{r Set up metadata}
geo.meta <- GEOquery::getGEO("GSE84465", destdir = "darmanis_data")
geo.meta <- data.frame(geo.meta[[1]]@phenoData@data)
```

### Make sure the samples and the metadata are in the same order.
```{r Order samples}
# Put samples in same order 
id.key <- readRDS(file.path("darmanis_data", "sample_id_key.RDS"))

# Make a sample conversion key
convert.key <- as.list(as.character(id.key$gsm.ids))
names(convert.key) <- as.character(id.key$run)

# Obtain GSM ids using conversion key and make these the column names 
colnames(dataset) <- dplyr::recode(colnames(dataset), !!!convert.key)

# Filter out samples not in the dataset
geo.meta <- geo.meta[match(colnames(dataset)[-1], geo.meta$geo_accession), ]
```

### Filtering genes and samples
Filter out genes that don't have at least one count in 1% of cells and filter out 
samples that don't express at least 100 genes. This function assumes the first 
column holds the genes.
```{r Filter out genes}
prepped.data <- GeneMatrixFilter(dataset, min_counts = 1, perc_genes = 0.01,
                                 num_genes = 100)
```

```{r Filter metadata to match}
# Only keep metadata for samples in the set
geo.meta <- geo.meta[match(colnames(prepped.data)[-1], geo.meta$geo_accession), ]
```

### Make batch file
In this case we will use the plate ids. 
If using ASAP, need the first column to be the sample IDs that match your sample data file
and the second column to be the corresponding batch the sample belongs to. 
I am excluding small batches here because they are difficult/impossible to evaluate 
for the purposes of clustering evaluations.
Here I excluded samples smaller than or equal to 2.
```{r Make batch info}
# Make batch info matrix
batch.info <- cbind(geo.meta$geo_accession, geo.meta$plate.id.ch1)

# Let's exclude batches of 2 or less
batch.sum <- summary(as.factor(batch.info[, 2]))
batch.small <- names(batch.sum)[which(batch.sum <= 2)]

# Make a regex statement to exclude these batches of one
regex <- paste0(paste0("^", batch.small, "$"), collapse = "|")
batch.info <- batch.info[ -grep(regex, batch.info[, 2]), ]

# Exclude these samples from the data
prepped.data <- prepped.data %>% dplyr::select("gene", batch.info[, 1])

# Look at a preview of what this looks like
head(batch.info)
```
```{r Filter metadata to match}
# Only keep metadata for samples in the set
geo.meta <- geo.meta[match(colnames(prepped.data)[-1], geo.meta$geo_accession), ]
```

### Write counts data file
```{r Write data file}
# Write a copy of the counts in the normalized data file
readr::write_tsv(prepped.data, file.path("darmanis_data", 
                                         "filtered_counts_darmanis.tsv"))
```

### Make batch file if you want to correct by batches
In this case we will use the plate ids. 
If using ASAP, need the first column to be the sample IDs that match your sample 
data file and the second column to be the corresponding batch the sample belongs to.
```{r Write batch file}
# Write batch info file for ASAP use
readr::write_tsv(data.frame(batch.info), 
                 file.path("darmanis_data", "batch.info.txt"))
```
### Write metadata info to be used later
```{r Write batch file}
# Write metadata file for subsequent testing
geo.meta %>% dplyr::select("geo_accession", "cell.type.ch1", "plate.id.ch1") %>%
             readr::write_tsv(file.path("darmanis_data", "metadata.tsv"))
```

Session info:
```{r}
sessionInfo()
```
