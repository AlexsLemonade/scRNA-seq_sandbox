---
title: "Clustering Statistics"
author: "C. Savonen CCDL for ALSF"
date: "12/17/2018"
output: html_document
---
### Purpose: compare normalization methods for a particular dataset
```{r Get packages if they aren't installed }
source("CZIA_functions.R")
if (!("Rtsne" %in% installed.packages())) {
   install.packages("Rtsne")
}
if (!("NMI" %in% installed.packages())) {
   install.packages("NMI")
}
if (!("caret" %in% installed.packages())) {
   install.packages("caret")
}
```

```{r Import Data}
# Get the file names of all the normalized files
normalized.files <- dir("../scRNA-seq_workflow/Darmanis_normalized")

# Read in each of the normalization files
normalized.data <- lapply(normalized.files, function(file) {
                              readr::read_tsv(file.path("../scRNA-seq_workflow/Darmanis_normalized",
                                                        file))
                          })

# Keep all the gene lists
genes <- lapply(normalized.data, function(x) x[,1])

# Keep the names with it.
names(normalized.data) <- gsub("\\.tab|Darmanis_", "", normalized.files)
```

```{r Metadata Set Up}
# Read in the data
meta <- readr::read_tsv(file.path("darmanis_data", "GSE84465_meta.tsv"))
```

```{r Retrieve cell types for these samples}
# Get sample names from columns
samples <- colnames(normalized.data[[1]])[-1]

# Keep metadata only for the samples we have
meta <- meta[match(samples, meta$geo_accession), ]

# Extra cell types info as it's own vector
cell.types <- meta$`cell type:ch1`
```

```{r Make clusters with tsne}
tsne <- lapply(normalized.data, function(dat) {
                        tsne.res <- Rtsne::Rtsne(t(dat[,-1]),
                                                 check_duplicates = FALSE)
                        tsne.res <- tsne.res$Y
                        names(tsne.res) <- samples
                        return(tsne.res)
                        })
```

```{r kmeans stats}
# Make iteration
iter <- 10
set.seed(1234)

# K means calculation 
kmeans.results <- lapply(tsne, function(feature) {

  # convert celltype into numbers
  celltype_num <- as.numeric(factor(cell.types))
  sample_id <- seq(1:nrow(feature))
  
  # iterative k-means
  nmi_score_all <- c()
  ari_score_all <- c()
  all_cluster <- list()
  
  for(i in 1:iter){
    # set k equal to the number of celltypes in the dataset
    k <- length(unique(cell.types))
    
    # perform k means clustering
    km <- kmeans(feature, k)

    # true clusters
    orignal_data <- data.frame(sample_id, celltype_num)
    
    # predicted clusters
    cl_data <- data.frame(sample_id, km$cluster)
    
    # calculate NMI and ARI score
    nmi_score <- NMI::NMI(orignal_data, cl_data)$value
    ari_score <- mclust::adjustedRandIndex(km$cluster, celltype_num)
    
    nmi_score_all <- c(nmi_score_all, nmi_score)
    ari_score_all <- c(ari_score_all, ari_score)
  }

  # Compile all results into a data.frame
  results <- data.frame(ari = ari_score_all, nmi = nmi_score_all)
  return(results)
})
```

```{r KNN eval}
knn_eval <- function(feature, celltype = cell.types, k = 5){
  # This function performs knn based evaluation 
  # Args:
  #  feature: a data.frame contains projected features (n dimensional space, n = 2, 3 ...), the columns are
  #    projected features in n dimensional space for a sample (cell), rows are samples
  #  celltype: vector contains cell type information
  #  k: cross validation fold
  # Returns:
  #  performance metrics and confusion matrix
  # Make the data into a data.frame:
  feature <- data.frame("tsne" = feature, cell.types = celltype)
  
  # Split observations into groups
  cv <- cvTools::cvFolds(nrow(feature), K = k, R = 1)
  
  # Create empty objects to store the performance information for each iteration
  perf.eval <- list()
  confusion.matrix <- 0
  
  # Go through this iteration that k times
  for (i in 1:k) {
    # Isolate samples for training the model
    train <- feature[cv$subsets[-which(cv$which == i)], ]
    
    # Isolate samples for testing the model
    test <- feature[cv$subsets[which(cv$which == i)], ]
    
    # Perform KNN model fitting
    knn.fit <- caret::train(cell.types~. , data = train, method = "knn",
                     trControl = caret::trainControl(method = "cv", number = 3),
                     preProcess = c("center", "scale"),
                     tuneLength = 10)
    
    # Evaluate the model
    knn.pred <- predict(knn.fit, newdata = subset(test, select = -c(cell.types)))
    perf.eval[[i]] <- round(cal_performance(knn.pred, test$cell.types, 3), 2)
    
    # Make the results into a matrix
    matrix <- as.matrix(table(test$cell.types, knn.pred, deparse.level = 0))
    confusion.matrix <- matrix + confusion.matrix
  }
  
  # Get mean performance of cross validation
  perf.eval <- dplyr::bind_rows(perf.eval)
  perf.mean <- as.data.frame(t(colMeans(perf.eval)))
  acc.sd <- sd(perf.eval$accuracy)
  perf.mean$acc.sd <- acc.sd
  
  return(list(performance = perf.mean, confusion = confusion.matrix))
}

knn.eval <- lapply(tsne, knn_eval)

```

```{r Format data for ggplot2}
# Transform list into a dataframe:
results <- unlist(all.results)
ggplot.data <- data.frame("data" = stringr::word(names(results), 1, sep = "\\."),
                          "test" = stringr::word(names(results), 2, sep = "\\."),
                          "mean" = unlist(results))

# Make variance it's own column 
var <- ggplot.data %>% dplyr::filter(grepl("var", ggplot.data$test)) %>%
                       dplyr::select(mean)

ggplot.data <- ggplot.data %>% dplyr::filter(!grepl("var", ggplot.data$test)) %>%
               dplyr::mutate(var = var$mean)
```

```{r Plot the means and variances}
ggplot(ggplot.data, aes(data, mean, test, fill = test)) +   
  geom_bar(position = position_dodge(.9), stat="identity") +
  geom_jitter(aes(ymin = mean-var, ymax = mean+var), width = 0.1,
                position = position_dodge(.9), stat="identity") +
  ylab("Mean") + 
  xlab("Normalization Method")

```

```{r}
sessionInfo()
```


